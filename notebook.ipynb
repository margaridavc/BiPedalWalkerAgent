{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Gym Environments and Implementing Reinforcement Learning Agents with Stable Baselines"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25cb727c855d83ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from training import latest_model\n",
    "from stable_baselines3 import PPO,A2C\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Trying the enviroment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52bbb4f3d1b1ce21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2', render_mode=\"rgb_array\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6811a21c02142c7b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "untrained_model = PPO(MlpPolicy, env, verbose=0)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(untrained_model, env, n_eval_episodes=100, warn=False)\n",
    "\n",
    "print(f\"mean_reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d5558ef03e14ada"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We created a script that creates a model and starts training it. If a model has already been created the script trains it further:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "859a06342aa6af24"
  },
  {
   "cell_type": "markdown",
   "source": [
    "```bash\n",
    "python training.py PPO\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "763fd7d227ea5618"
  },
  {
   "cell_type": "markdown",
   "source": [
    "```bash\n",
    "python training.py A2C\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5ee6713421227d6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The models are saved in the folder `models` and then we use the latest model to test them"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bb44e852f824c52"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To see the training progress we can use tensorboard:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bff183f09ee8f79d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "```bash\n",
    "tensorboard --logdir=logs\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "deab54558d1785fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "PPO algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c3c9589925fc5b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ppo_model = PPO.load(latest_model(\"PPO\"), env=env)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4193c862c8885d13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "episodes = 100\n",
    "obs, info = env.reset()\n",
    "for ep in range(episodes):\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = ppo_model.predict(obs)\n",
    "        obs, rewards, done,_, info = env.step(action)\n",
    "        env.render()\n",
    "        print(rewards)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f34b0218a8d978b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "A2C algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2b7bd720bc6628c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a2c_model = A2C.load(latest_model(\"A2C\"), env=env)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2aa7fa3f10488e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "episodes = 100\n",
    "obs, info = env.reset()\n",
    "for ep in range(episodes):\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = a2c_model.predict(obs)\n",
    "        obs, rewards, done,_, info = env.step(action)\n",
    "        env.render()\n",
    "        print(rewards)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "215a44b7e8c8cafa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the original models and with the original env we got these results:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84dd1432c698a2be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![graph](imgs/PPOoriginalvsA2Coriginal.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44962a2e317a24c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see the PPO algorithm is better than the A2C algorithm, at least with the original models and the original env in the time we've trained them.\n",
    "\n",
    "We can also see that the A2C model trained faster than the PPO model.\n",
    "\n",
    "To see if we can improve the results we will try to create a RewardWrapper to better reward the agent."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cb3972dd5dada6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "922169ba884a9a28"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tuning Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "380917c24bd8a991"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1879ca478b3539ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fc806f0f10e52420"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ca5f160fef2ec4ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "839ad38eb41c9705"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1918a7960ef01169"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b5790e48bae91594"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cad31ad20de73c4f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "73243d24014ff015"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2ef10eff65299656"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
