{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Gym Environments and Implementing Reinforcement Learning Agents with Stable Baselines"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25cb727c855d83ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO, A2C, SAC\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('BipedalWalker-v3')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1ae42faa11e2d44"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We chose the BipedalWalker-v3 environment because it is a continuous action space, and it is a challenging environment to solve."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52bbb4f3d1b1ce21"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We chose the PPO, A2C and SAC algorithms to train our agents because ...\n",
    "\n",
    "Let's test the environment with a random agent from each algorithm:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c7fc1f9cb98cc40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_performance(model):\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100, warn=False)\n",
    "\n",
    "    print(f\"mean_reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86aacd37b81217b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "untrained_model = PPO(\"MlpPolicy\", env, verbose=0)\n",
    "\n",
    "test_performance(untrained_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d5558ef03e14ada"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "untrained_model = A2C(\"MlpPolicy\", env, verbose=0)\n",
    "\n",
    "test_performance(untrained_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68d6064d88482a23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "untrained_model = SAC(\"MlpPolicy\", env, verbose=0)\n",
    "\n",
    "test_performance(untrained_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b1a8b29f8b757b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the random agents are not good at all.\n",
    "\n",
    "Now let's use them to properly create a model and train them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fcaed937df32e88"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the models without changes to the environment or the models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa84561803050502"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We created a script that creates a model and starts training it. If a model has already been created, the script trains it further\n",
    "\n",
    "To train the models, we created the environment the following code:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "859a06342aa6af24"
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "import os\n",
    "from sys import argv\n",
    "\n",
    "from stable_baselines3 import A2C, PPO, SAC\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "env_id = \"BipedalWalker-v3\"\n",
    "\n",
    "TIMESTEPS = 100000\n",
    "models_dir = \"models\"\n",
    "logdir = \"logs\"\n",
    "\n",
    "\n",
    "def latest_model(algorithm):\n",
    "    models = [int(model.split(\".\")[0]) for model in os.listdir(f\"{models_dir}/{algorithm}\")]\n",
    "    models.sort()\n",
    "    return f\"{models_dir}/{algorithm}/{models[-1]}.zip\"\n",
    "\n",
    "\n",
    "def train_model(algo, algo_name, policy, n_envs):\n",
    "    env = make_vec_env(env_id, n_envs=n_envs, vec_env_cls=SubprocVecEnv, env_kwargs=dict(hardcore=False),\n",
    "                       vec_env_kwargs=dict(start_method='fork'))\n",
    "\n",
    "    if os.path.exists(f\"{models_dir}/{algo_name}\"):\n",
    "        if os.listdir(f\"{models_dir}/{algo_name}\"):\n",
    "\n",
    "            model_path = latest_model(algo_name)\n",
    "            model = algo.load(model_path, env=env)\n",
    "            iters = int(int(model_path.split(\"/\")[2].split(\".\")[0]) / 10 ** 4)\n",
    "        else:\n",
    "            model = algo(policy, env, verbose=1,tensorboard_log=logdir)\n",
    "            iters = 0\n",
    "    else:\n",
    "        os.makedirs(f\"{models_dir}/{algo_name}\")\n",
    "        model = algo(policy, env, verbose=1,tensorboard_log=logdir)\n",
    "        iters = 0\n",
    "\n",
    "    while True:\n",
    "        iters += 1\n",
    "        model.learn(total_timesteps=TIMESTEPS, progress_bar=True, reset_num_timesteps=False, tb_log_name=algo_name) \n",
    "        model.save(f\"{models_dir}/{algo_name}/{TIMESTEPS * iters}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        if len(argv) != 2:\n",
    "            raise ValueError(\"No arguments given. Please specify which model to train.\")\n",
    "\n",
    "        if not os.path.exists(logdir):\n",
    "            os.makedirs(logdir)\n",
    "\n",
    "        model_type = argv[1]\n",
    "\n",
    "        if model_type == \"A2C\":\n",
    "            train_model(A2C, model_type, \"MlpPolicy\", n_envs=3)\n",
    "        elif model_type == \"PPO\":\n",
    "            train_model(PPO, model_type, \"MlpPolicy\", n_envs=5)\n",
    "        elif model_type == \"SAC\":\n",
    "            train_model(SAC, model_type, \"MlpPolicy\", n_envs=20)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid argument. Please specify a valid algorithm.\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a65265f32d7f14"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To train each model, we then used:\n",
    "\n",
    "\n",
    "```bash\n",
    "    python training.py PPO\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```bash\n",
    "    python training.py A2C\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```bash\n",
    "    python training.py SAC\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eff268ae05dc3d6a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The models were then saved in a folder `models`. For the demonstration we chose the most advanced models and placed them inside the `final_models` folder."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bb44e852f824c52"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To see the training progress, we can use tensorboard:\n",
    "\n",
    "\n",
    "\n",
    "```bash\n",
    "    tensorboard --logdir=logs\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "deab54558d1785fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the mean reward graph for the training of the models in easy difficulty mode:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5ae239ded70c61b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![graph](imgs/original_models_original_env_easy.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44962a2e317a24c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the PPO and SAC algorithms rise quickly in the first 2M iterations, while the A2C algorithm stays at the same.\n",
    "\n",
    "The PPO and SAC algorithms reach the top reward (+-300) in the first 4/6M iterations, while the A2C algorithm even in the 24M iterations does not reach the top reward.\n",
    "\n",
    "Now we tested the models in the hardcore difficulty mode. To do that, in the training script we changed in the environment creation: `hardcore=True`\n",
    "\n",
    "Here is the mean reward graph for the training of the models in hardcore difficulty mode:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cb3972dd5dada6c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![graph](imgs/original_models_original_env.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61fcadfb54908299"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CONCLUSIONS ABOUT THE TRAINING CHANGE LATER"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3605c92c7152b49"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's see how they perform:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4583bbf36fc5cbd7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "From now on, we are going to only use the hardcore mode."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e997481fba2d7fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### PPO algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c3c9589925fc5b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ppo_model = PPO.load(\"final_models/original_PPO_original_env.zip\", env=env)\n",
    "\n",
    "test_performance(ppo_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f34b0218a8d978b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### A2C algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2b7bd720bc6628c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a2c_model = A2C.load(\"final_models/original_A2C_original_env.zip\", env=env)\n",
    "\n",
    "test_performance(a2c_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "215a44b7e8c8cafa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SAC algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "faf76042f58d9522"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sac_model = SAC.load(\"final_models/original_SAC_original_env.zip\", env=env)\n",
    "\n",
    "test_performance(sac_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "937f8c2e1e8cf07b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the models with RewardWrapper(env) and without Hyperparameters changes to the models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25f13c43c866b867"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To see if we can improve the results, we will try to create a RewardWrapper to better reward the agent.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "\n",
    "from gymnasium import RewardWrapper as RW\n",
    "\n",
    "\n",
    "class RewardWrapper(RW):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "\n",
    "    def step(self, action):\n",
    "        # Perform the environment step\n",
    "        obs, reward, done, _, info = self.env.step(action)\n",
    "\n",
    "        # Add a reward for keeping balance\n",
    "        # obs[2] is the angle of the agent from the vertical position\n",
    "        balance_reward = abs(obs[2])\n",
    "        reward += balance_reward\n",
    "\n",
    "        return obs, reward, done, _, info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1821ba795abc8afe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This reward wrapper gives a reward to the agent for keeping balance.\n",
    "\n",
    "To train the models, with the RewardWrapper we need to change the environment creation in the training script:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5049faf83399ec2d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "env = make_vec_env(env_id, n_envs=NUM_ENVS, wrapper_class=RewardWrapper, vec_env_cls=SubprocVecEnv, vec_env_kwargs=dict(start_method='fork'), env_kwargs=dict(hardcore=True))\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb8da1239d949338"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's train the models again and check the results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f4154c11dc240a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![graph](imgs/original_models_wrapped_env.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3d426d2823b84ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CONCLUSIONS ABOUT THE TRAINING CHANGE LATER"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1522ee57fe3b9e1f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's see how they perform:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcac235b1c370c88"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PPO algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ac7fb3bff69345f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ppo_model = PPO.load(\"final_models/original_PPO_wrapped_env.zip\", env=env)\n",
    "\n",
    "test_performance(ppo_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "632a0e9c84edb603"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A2C algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd8e4e53086bfd76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a2c_model = A2C.load(\"final_models/original_A2C_wrapped_env.zip\", env=env)\n",
    "\n",
    "test_performance(a2c_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d34b1838817bcca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SAC algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94420a5b99a82c8e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sac_model = SAC.load(\"final_models/original_SAC_wrapped_env.zip\", env=env)\n",
    "\n",
    "test_performance(sac_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37ff9956b9ec1925"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's see the training progress:\n",
    "\n",
    "![graph](imgs/original_models_wrapped_env.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f1394ad309deee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CONCLUSIONS ABOUT THE TRAINING CHANGE LATER"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff678363f48966ba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tuning Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "380917c24bd8a991"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We turned the hyperparameters of the models to see if we could improve the results.\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "    model = algo(policy, env, verbose=1, tensorboard_log=logdir)  # (hyperparameter) ,learning_rate=0.0001, (Neural Network Architecture change)(Neural Network Architecture change) policy_kwargs=dict(net_arch=[256,(...n_layers...), 256]))\n",
    "    \n",
    "\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=algo_name)  # (hyperparameters) , batch_size=256, ent_coef=0.01, vf_coef=0.5, gae_lambda=0.95))\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9c314781f84cbc9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PPO algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fe4d5e271344c05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ppo_model = PPO.load(\"final_models/tunned_PPO_wrapped_env.zip\", env=env)\n",
    "\n",
    "test_performance(ppo_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "faa29e452c49e23f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A2C algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83083646e1cbc72c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a2c_model = A2C.load(\"final_models/tunned_A2C_wrapped_env.zip\", env=env)\n",
    "\n",
    "test_performance(a2c_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd722f47d59a5a16"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SAC algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98e61f0b2d13eea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sac_model = SAC.load(\"final_models/tunned_SAC_wrapped_env.zip\", env=env)\n",
    "\n",
    "test_performance(sac_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e94cc53deaa739d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's see the training progress:\n",
    "\n",
    "![graph](imgs/tunned_models_wrapped_env.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37e1711ab4f50c6c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CONCLUSIONS ABOUT THE TRAINING CHANGE LATER"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38cb3152471ae85"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualising the best model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d354aaa72e291af1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's visualize the performance of the best model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7e924a2579faafa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize_model():\n",
    "    test_env =  gym.make('BipedalWalker-v3', hardcore=True, render_mode=\"human\")\n",
    "    \n",
    "    model = PPO.load(\"final_models/tunned_PPO_wrapped_env.zip\", env=test_env)\n",
    "    obs, info = test_env.reset()\n",
    "    \n",
    "    while True:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, terminated, truncated, info = test_env.step(action)\n",
    "        test_env.render()\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "        \n",
    "# visualize_model()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5790e48bae91594"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cad31ad20de73c4f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "73243d24014ff015"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2ef10eff65299656"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
