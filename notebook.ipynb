{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Gym Environments and Implementing Reinforcement Learning Agents with Stable Baselines\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## The environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25cb727c855d83ec"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env_id = \"BipedalWalker-v3\"\n",
    "env = gym.make(env_id, hardcore=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T04:42:16.755515Z",
     "start_time": "2023-12-14T04:42:16.638835Z"
    }
   },
   "id": "f1ae42faa11e2d44"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We chose the `BipedalWalker-v3` environment because it's a continuous action space, and it's (when in hardcore mode) a challenging environment to solve.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "To see the training progress, we use tensorboard to visualize the training progress and took screenshots of the graphs to use them below. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "```bash\n",
    "    tensorboard --logdir=logs\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "## The algorithms\n",
    "\n",
    "\n",
    "In the assignment, to use our environment, we needed to choose a few from the following algorithms:\n",
    "\n",
    "<br>\n",
    "\n",
    "- ARS (Augmented Random Search)\n",
    "- A2C (Advantage Actor Critic)\n",
    "- DDPG (Deep Deterministic Policy Gradient)\n",
    "- PPO (Proximal Policy Optimization)\n",
    "- RecurrentPPO (Recurrent Proximal Policy Optimization)\n",
    "- SAC (Soft Actor Critic)\n",
    "- TD3 (Twin Delayed DDPG)\n",
    "- TQC (Twin Q-Value Critic)\n",
    "- TRPO (Trust Region Policy Optimization)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "To see the top 3 algorithms, we trained all of them and chose the best ones:\n",
    "\n",
    "To train them, we used the following script:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c7fc1f9cb98cc40"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "models_dir = \"models\"\n",
    "logdir = \"logs\"\n",
    "TIMESTEPS = 10**6\n",
    "\n",
    "\n",
    "def train_model(algo, algo_name, policy, n_envs=os.cpu_count()):\n",
    "    \n",
    "    def latest_model(algorithm):\n",
    "        models = [int(m.split(\".\")[0]) for m in os.listdir(f\"{models_dir}/{algorithm}\")]\n",
    "        models.sort()\n",
    "        return f\"{models_dir}/{algorithm}/{models[-1]}.zip\"\n",
    "\n",
    "\n",
    "    if n_envs == 1:\n",
    "        train_env = DummyVecEnv([lambda: gym.make(env_id, hardcore=True)])\n",
    "    else:\n",
    "        train_env = make_vec_env(env_id, n_envs=n_envs, vec_env_cls=SubprocVecEnv, env_kwargs=dict(hardcore=True),\n",
    "                           vec_env_kwargs=dict(start_method='fork'))\n",
    "\n",
    "    if os.path.exists(f\"{models_dir}/{algo_name}\"):\n",
    "        if os.listdir(f\"{models_dir}/{algo_name}\"):\n",
    "\n",
    "            model_path = latest_model(algo_name)\n",
    "            model = algo.load(model_path, env=train_env)\n",
    "            iters = int(int(model_path.split(\"/\")[2].split(\".\")[0]) / 10 ** 4)\n",
    "        else:\n",
    "\n",
    "            model = algo(policy, train_env, verbose=1, tensorboard_log=logdir)\n",
    "            iters = 0\n",
    "    else:\n",
    "        os.makedirs(f\"{models_dir}/{algo_name}\")\n",
    "        model = algo(policy, train_env, verbose=1,tensorboard_log=logdir)\n",
    "        iters = 0\n",
    "\n",
    "    while True:\n",
    "        iters += 1\n",
    "        model.learn(total_timesteps=TIMESTEPS, progress_bar=True, reset_num_timesteps=False,tb_log_name=algo_name)\n",
    "        model.save(f\"{models_dir}/{algo_name}/{TIMESTEPS * iters}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T04:43:08.893128Z",
     "start_time": "2023-12-14T04:43:08.884625Z"
    }
   },
   "id": "5857e123a0e11115"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training all the possible models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ad91d413e67960f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To train the models, we called the funtion above.\n",
    "\n",
    "Then we got this graph of the training progress:\n",
    "\n",
    "![graph](imgs/all_models_original_env.png)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fb720ccfe67f156"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import random\n",
    "\n",
    "def test_performance(model):\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100, warn=False)\n",
    "\n",
    "    return mean_reward"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T04:43:09.308946Z",
     "start_time": "2023-12-14T04:43:09.299523Z"
    }
   },
   "id": "86aacd37b81217b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's test the environment with a random agent from each algorithm:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "799d52ccff74f4c8"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C,DDPG, PPO, SAC, TD3\n",
    "from sb3_contrib import ARS, RecurrentPPO, TQC, TRPO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "performances = [(\"A2C\", test_performance(A2C(\"MlpPolicy\", env, verbose=0))),\n",
    "                (\"ARS\", test_performance(ARS(\"MlpPolicy\", env, verbose=0))),\n",
    "                (\"DDPG\", test_performance(DDPG(\"MlpPolicy\", env, verbose=0))),\n",
    "                (\"PPO\", test_performance(PPO(\"MlpPolicy\", env, verbose=0))),\n",
    "                (\"QRDQN\", test_performance(QRDQN(\"MlpPolicy\", env, verbose=0))),\n",
    "                (\"RecurrentPPO\", test_performance(RecurrentPPO(\"MlpLstmPolicy\", env))),\n",
    "                (\"SAC\", test_performance(SAC(\"MlpPolicy\", env, verbose=0))),\n",
    "                (\"TD3\", test_performance(TD3(\"MlpPolicy\", env, verbose=0))),\n",
    "                (\"TQC\", test_performance(TQC(\"MlpPolicy\", env, verbose=0))),\n",
    "                (\"TRPO\", test_performance(TRPO(\"MlpPolicy\", env, verbose=0)))]\n",
    "\n",
    "names, values = zip(*performances)\n",
    "cmap = plt.get_cmap('Greens_r')\n",
    "colors = [cmap(i / len(names)) for i in range(len(names))]\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.bar(names, values, color=colors, alpha=0.7)\n",
    "ax.set_title('Performance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T04:44:45.210145Z",
     "start_time": "2023-12-14T04:44:45.011939Z"
    }
   },
   "id": "ad543c044d2f62ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the top 5\n",
    "\n",
    "Now let's continue the training with the top five algorithms:\n",
    "\n",
    "- top1\n",
    "- top2\n",
    "- top3\n",
    "- top4\n",
    "- top5\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## Training the models without changes to the environment or the models\n",
    "\n",
    "\n",
    "Just to do a little sanity check, we are going to train with `hardcore=False`.\n",
    "\n",
    "\n",
    "Here is the mean reward graph for the training of the models in the easy mode:\n",
    "\n",
    "<br>\n",
    "\n",
    "![graph](imgs/original_models_original_env_easy.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "We can see that the PPO and SAC algorithms rise quickly in the first 2M iterations, while the A2C algorithm stays at the same.\n",
    "\n",
    "The PPO and SAC algorithms reach the top reward (+-300) in the first 4/6M iterations, while the A2C algorithm even in the 24M iterations does not reach the top reward.\n",
    "\n",
    "Now we tested the models in the hardcore difficulty mode. To do that, in the training script we changed in the environment creation: `hardcore=True`\n",
    "\n",
    "Here is the mean reward graph for the training of the models in hardcore difficulty mode:\n",
    "\n",
    "<br>\n",
    "\n",
    "![graph](imgs/original_models_original_env.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "We can see after we turned on hardcore mode, we saw a drop in the performance of the models. The PPO and SAC algorithms still rise quickly in the 10M iterations, while the A2C algorithm more or less stays at the same."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cb3972dd5dada6c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's see how they perform:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4583bbf36fc5cbd7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "From now on, we are going to only use the hardcore mode."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e997481fba2d7fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### PPO algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c3c9589925fc5b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ppo_model = PPO.load(\"final_models/original_PPO_original_env.zip\", env=env)\n",
    "\n",
    "test_performance(ppo_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f34b0218a8d978b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### A2C algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2b7bd720bc6628c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a2c_model = A2C.load(\"final_models/original_A2C_original_env.zip\", env=env)\n",
    "\n",
    "test_performance(a2c_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "215a44b7e8c8cafa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SAC algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "faf76042f58d9522"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sac_model = SAC.load(\"final_models/original_SAC_original_env.zip\", env=env)\n",
    "\n",
    "test_performance(sac_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "937f8c2e1e8cf07b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the models with RewardWrapper(env) and without Hyperparameters changes to the models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25f13c43c866b867"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We checked, and the problem is that the walker gets stuck in the same place with it's legs open. \n",
    "\n",
    "\n",
    "![graph](imgs/stuck_walker.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1a2cddca309b8b2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To see if we can improve the results, we will try to reward the agent when they stay balanced upright.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "\n",
    "from gymnasium import RewardWrapper as RW\n",
    "\n",
    "\n",
    "class RewardWrapper(RW):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "\n",
    "    def step(self, action):\n",
    "        # Perform the environment step\n",
    "        obs, reward, done, _, info = self.env.step(action)\n",
    "\n",
    "        # Add a reward for keeping balance\n",
    "        # obs[2] is the angle of the agent from the vertical position\n",
    "        balance_reward = abs(obs[2])\n",
    "        reward += balance_reward\n",
    "\n",
    "        return obs, reward, done, _, info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1821ba795abc8afe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To train the models, with the RewardWrapper we need to change the environment creation in the training script:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5049faf83399ec2d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "env = make_vec_env(env_id, n_envs=NUM_ENVS, wrapper_class=RewardWrapper, vec_env_cls=SubprocVecEnv, vec_env_kwargs=dict(start_method='fork'), env_kwargs=dict(hardcore=True))\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb8da1239d949338"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's train the models again and check the results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f4154c11dc240a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![graph](imgs/original_models_wrapped_env.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3d426d2823b84ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "After aplying our RewardWrapper, we can see that in the 50M iterations, the PPO learned very well and reached positive rewards.\n",
    "\n",
    "\n",
    "We can see that the SAC algorithm stayed the same as before the wrapper. \n",
    "\n",
    "And finnaly the A2C algorithm learned a little bit. Still not reaching positive rewards but its better than before."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1522ee57fe3b9e1f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's see how they perform:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcac235b1c370c88"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PPO algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ac7fb3bff69345f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ppo_model = PPO.load(\"final_models/original_PPO_wrapped_env.zip\", env=env)\n",
    "\n",
    "test_performance(ppo_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "632a0e9c84edb603"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A2C algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd8e4e53086bfd76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a2c_model = A2C.load(\"final_models/original_A2C_wrapped_env.zip\", env=env)\n",
    "\n",
    "test_performance(a2c_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d34b1838817bcca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SAC algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94420a5b99a82c8e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sac_model = SAC.load(\"final_models/original_SAC_wrapped_env.zip\", env=env)\n",
    "\n",
    "test_performance(sac_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37ff9956b9ec1925"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tuning Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "380917c24bd8a991"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We turned the hyperparameters of the models to see if we could improve the results.\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "    model = algo(policy, env, verbose=1, tensorboard_log=logdir)  # (hyperparameter) ,learning_rate=0.0001, (Neural Network Architecture change)(Neural Network Architecture change) policy_kwargs=dict(net_arch=[256,(...n_layers...), 256]))\n",
    "    \n",
    "\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=algo_name)  # (hyperparameters) , batch_size=256, ent_coef=0.01, vf_coef=0.5, gae_lambda=0.95))\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9c314781f84cbc9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PPO algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fe4d5e271344c05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ppo_model = PPO.load(\"final_models/tunned_PPO_wrapped_env.zip\", env=env)\n",
    "\n",
    "test_performance(ppo_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "faa29e452c49e23f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A2C algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83083646e1cbc72c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a2c_model = A2C.load(\"final_models/tunned_A2C_wrapped_env.zip\", env=env)\n",
    "\n",
    "test_performance(a2c_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd722f47d59a5a16"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SAC algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98e61f0b2d13eea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sac_model = SAC.load(\"final_models/tunned_SAC_wrapped_env.zip\", env=env)\n",
    "\n",
    "test_performance(sac_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e94cc53deaa739d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's see the training progress:\n",
    "\n",
    "![graph](imgs/tunned_models_wrapped_env.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37e1711ab4f50c6c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CONCLUSIONS ABOUT THE TRAINING CHANGE LATER"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38cb3152471ae85"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualising the best model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d354aaa72e291af1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's visualize the performance of the best model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7e924a2579faafa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize_model():\n",
    "    test_env =  gym.make('BipedalWalker-v3', hardcore=True, render_mode=\"human\")\n",
    "    \n",
    "    model = PPO.load(\"final_models/original_PPO_wrapped_env.zip\", env=test_env)\n",
    "    obs, info = test_env.reset()\n",
    "    \n",
    "    while True:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, terminated, truncated, info = test_env.step(action)\n",
    "        test_env.render()\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "        \n",
    "visualize_model()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5790e48bae91594"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cad31ad20de73c4f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "73243d24014ff015"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2ef10eff65299656"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
