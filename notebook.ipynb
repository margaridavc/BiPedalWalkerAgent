{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Gym Environments and Implementing Reinforcement Learning Agents with Stable Baselines"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25cb727c855d83ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO,A2C\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1ae42faa11e2d44"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We chose the CarRacing-v2 environment because it is a continuous action space, and it is a challenging environment to solve.\n",
    "\n",
    "To test ourselves, we created a script that allows us to play the game with the keyboard:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52bbb4f3d1b1ce21"
  },
  {
   "cell_type": "markdown",
   "source": [
    "```bash\n",
    "    python human_test.py\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cb2f9e86df9c025"
  },
  {
   "cell_type": "markdown",
   "source": [
    "WE chose the PPO and A2C algorithms to train our agents because they are the most used algorithms in the stable baselines3 library.\n",
    "\n",
    "Let's test the environment with a random agent from each algorithm:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c7fc1f9cb98cc40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_performance(model):\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100, warn=False)\n",
    "\n",
    "    print(f\"mean_reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86aacd37b81217b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "untrained_model = PPO(MlpPolicy, env, verbose=0)\n",
    "\n",
    "test_performance(untrained_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d5558ef03e14ada"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "untrained_model = A2C(MlpPolicy, env, verbose=0)\n",
    "\n",
    "test_performance(untrained_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68d6064d88482a23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the random agents are not good at all.\n",
    "\n",
    "Now let's use them to properly create a model and train them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fcaed937df32e88"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the models without changes to the environment or the models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa84561803050502"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We created a script that creates a model and starts training it. If a model has already been created, the script trains it further\n",
    "\n",
    "To train the models, we created the environment the following code:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "859a06342aa6af24"
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "import os\n",
    "from sys import argv\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.a2c import MlpPolicy as A2C_MlpPolicy\n",
    "from stable_baselines3.ppo import MlpPolicy as PPO_MlpPolicy\n",
    "from NewReward import RewardWrapper\n",
    "\n",
    "env_id = \"CarRacing-v2\"\n",
    "TIMESTEPS = 100000\n",
    "models_dir = \"models\"\n",
    "logdir = \"logs\"\n",
    "NUM_ENVS = os.cpu_count()\n",
    "\n",
    "def train_model(algo, algo_name, policy, env):\n",
    "    if os.path.exists(f\"{models_dir}/{algo_name}\"):\n",
    "        if os.listdir(f\"{models_dir}/{algo_name}\"):\n",
    "            model_path = latest_model(algo_name)\n",
    "            model = algo.load(model_path, env=env)\n",
    "            iters = int(int(model_path.split(\"/\")[2].split(\".\")[0]) / 10 ** 4)\n",
    "        else:\n",
    "\n",
    "\n",
    "            model = algo(policy, env, verbose=1, tensorboard_log=logdir)\n",
    "            iters = 0\n",
    "    else:\n",
    "        os.makedirs(f\"{models_dir}/{algo_name}\")\n",
    "        model = algo(policy, env, verbose=1, tensorboard_log=logdir)\n",
    "        iters = 0\n",
    "\n",
    "    while True:\n",
    "        iters += 1\n",
    "        model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=algo_name)\n",
    "        model.save(f\"{models_dir}/{algo_name}/{TIMESTEPS * iters}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        if len(argv) != 2:\n",
    "            raise ValueError(\"No arguments given. Please specify which model to train.\")\n",
    "\n",
    "        if not os.path.exists(logdir):\n",
    "            os.makedirs(logdir)\n",
    "\n",
    "        model_type = argv[1]\n",
    "\n",
    "        env = make_vec_env(env_id, n_envs=NUM_ENVS, vec_env_cls=SubprocVecEnv,\n",
    "                           vec_env_kwargs=dict(start_method='fork'))\n",
    "\n",
    "        if model_type == \"A2C\":\n",
    "            train_model(A2C, model_type, A2C_MlpPolicy, env)\n",
    "        elif model_type == \"PPO\":\n",
    "            train_model(PPO, model_type, PPO_MlpPolicy, env)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid argument. Please specify 'A2C' or 'PPO'.\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a65265f32d7f14"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To train each model, we then used:\n",
    "\n",
    "\n",
    "```bash\n",
    "    python training.py PPO\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```bash\n",
    "    python training.py A2C\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eff268ae05dc3d6a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The models were then saved in a folder `models`. For the demonstration we chose the most advanced models and placed them inside the `final_models` folder."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bb44e852f824c52"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To see the training progress, we can use tensorboard:\n",
    "\n",
    "\n",
    "\n",
    "```bash\n",
    "    tensorboard --logdir=logs\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "deab54558d1785fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![graph](imgs/original_models_original_env.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44962a2e317a24c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the original models and the original environment in the time we've trained them, we can see the PPO algorithm is better than the A2C algorithm.\n",
    "\n",
    "Though we can also see that the A2C model trained faster than the PPO model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cb3972dd5dada6c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's see how they perform:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4583bbf36fc5cbd7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### PPO algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c3c9589925fc5b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ppo_model = PPO.load(\"final_models/original_PPO_original_env.zip\", env=env)\n",
    "\n",
    "test_performance(ppo_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f34b0218a8d978b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### A2C algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2b7bd720bc6628c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a2c_model = A2C.load(\"final_models/original_A2C_original_env.zip\", env=env)\n",
    "\n",
    "test_performance(a2c_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "215a44b7e8c8cafa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the models with RewardWrapper(env) and without changes to the models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25f13c43c866b867"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To see if we can improve the results, we will try to create a RewardWrapper to better reward the agent.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "    from gymnasium import RewardWrapper as RW\n",
    "    \n",
    "    \n",
    "    class RewardWrapper(RW):\n",
    "        def __init__(self, env):\n",
    "            super().__init__(env)\n",
    "            self.total_reward = 25\n",
    "    \n",
    "        def reset(self, **kwargs):\n",
    "            self.total_reward = 0\n",
    "            return self.env.reset(**kwargs)\n",
    "    \n",
    "        def step(self, action):\n",
    "            # Perform the environment step\n",
    "            obs, reward, done, _, info = self.env.step(action)\n",
    "    \n",
    "            self.total_reward += reward\n",
    "    \n",
    "            if self.total_reward < 0:\n",
    "                done = True\n",
    "    \n",
    "            return obs, reward, done, _, info\n",
    "\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1821ba795abc8afe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We noticed that the episode only ended when the car went out of the map.\n",
    "\n",
    "In the training, we also noticed that the car was getting stuck doing circles.\n",
    "\n",
    "When the car is not doing progress, there is a penalty of -0.1.\n",
    "\n",
    "So we decided to end the episode when the total reward was negative. Meaning that the car was not making any progress for a long time.\n",
    "\n",
    "To give the model time to start gaining total reward, we decided to start the total reward at 25.\n",
    " \n",
    "This way, the model has time to start making progress without the episode ending too soon.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5049faf83399ec2d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To train the models, with the RewardWrapper we need to change the environment creation in the training script:\n",
    "\n",
    "\n",
    "```python\n",
    "env = make_vec_env(env_id, n_envs=NUM_ENVS, wrapper_class=RewardWrapper, vec_env_cls=SubprocVecEnv, vec_env_kwargs=dict(start_method='fork'))\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb8da1239d949338"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PPO algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ac7fb3bff69345f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ppo_model = PPO.load(\"final_models/original_PPO_wrapped_env.zip\", env=env)\n",
    "\n",
    "test_performance(ppo_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "632a0e9c84edb603"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A2C algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd8e4e53086bfd76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a2c_model = A2C.load(\"final_models/original_A2C_wrapped_env.zip\", env=env)\n",
    "\n",
    "test_performance(a2c_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d34b1838817bcca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's see the training progress:\n",
    "\n",
    "![graph](imgs/original_models_wrapped_env.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f1394ad309deee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tuning Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "380917c24bd8a991"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We turned the hyperparameters of the models to see if we could improve the results.\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "    model = algo(policy, env, verbose=1, tensorboard_log=logdir)  # (hyperparameter) ,learning_rate=0.0001, (Neural Network Architecture change)(Neural Network Architecture change) policy_kwargs=dict(net_arch=[256,(...n_layers...), 256]))\n",
    "    \n",
    "\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=algo_name)  # (hyperparameters) , batch_size=256, ent_coef=0.01, vf_coef=0.5, gae_lambda=0.95))\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9c314781f84cbc9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PPO algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fe4d5e271344c05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ppo_model = PPO.load(\"final_models/tunned_PPO_wrapped_env.zip\", env=env)\n",
    "\n",
    "test_performance(ppo_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "faa29e452c49e23f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A2C algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83083646e1cbc72c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a2c_model = A2C.load(\"final_models/tunned_A2C_wrapped_env.zip\", env=env)\n",
    "\n",
    "test_performance(a2c_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd722f47d59a5a16"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's see the training progress:\n",
    "\n",
    "![graph](imgs/tunned_models_wrapped_env.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37e1711ab4f50c6c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualising the best model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d354aaa72e291af1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's visualize the performance of the best model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7e924a2579faafa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_env = gym.make('CarRacing-v2',render_mode='human')\n",
    "test_model = PPO.load(\"final_models/tunned_PPO_wrapped_env.zip\", env=test_env)\n",
    "obs, info = env.reset()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action, _states = test_model.predict(obs)\n",
    "    obs, rewards, done, _, info = test_env.step(action)\n",
    "    env.render()\n",
    "    print(rewards)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5790e48bae91594"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cad31ad20de73c4f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "73243d24014ff015"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2ef10eff65299656"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
